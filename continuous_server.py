import asyncio
import websockets
import json
import os
import csv
import time
import numpy as np
from collections import deque
from services.continuous_predict import predict_sequence_from_frames

connected_clients = set()

# File log
CSV_FILE = "frames_log.csv"

# N·∫øu file ch∆∞a t·ªìn t·∫°i, ghi header
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, mode="w", newline="") as f:
        writer = csv.writer(f)
        header = [i for i in range(126)]
        writer.writerow(header)

class ContinuousPredictionManager:
    def __init__(self):
        self.client_buffers = {}  # L∆∞u buffer cho m·ªói client
        self.client_last_predictions = {}  # Cache k·∫øt qu·∫£ prediction cu·ªëi
        self.client_prediction_times = {}  # Track th·ªùi gian prediction
        self.prediction_queue = asyncio.Queue()  # Queue x·ª≠ l√Ω batch prediction
        self.batch_processor_task = None
        
        # C·∫•u h√¨nh t·ªëi ∆∞u
        self.MIN_PREDICTION_INTERVAL = 0.3  # 300ms minimum gi·ªØa c√°c prediction
        self.BATCH_SIZE = 4  # X·ª≠ l√Ω 4 client c√πng l√∫c
        self.CONFIDENCE_THRESHOLD = 0.9
        self.DUPLICATE_THRESHOLD = 0.95  # Threshold ƒë·ªÉ coi l√† duplicate
        
    async def start_batch_processor(self):
        """Kh·ªüi ƒë·ªông batch processor ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu prediction c√πng l√∫c"""
        if self.batch_processor_task is None:
            self.batch_processor_task = asyncio.create_task(self._batch_processor())
            
    async def stop_batch_processor(self):
        """D·ª´ng batch processor"""
        if self.batch_processor_task:
            self.batch_processor_task.cancel()
            try:
                await self.batch_processor_task
            except asyncio.CancelledError:
                pass
            self.batch_processor_task = None
    
    async def _batch_processor(self):
        """X·ª≠ l√Ω batch prediction ƒë·ªÉ t·ªëi ∆∞u GPU/CPU"""
        pending_predictions = []
        
        while True:
            try:
                # Collect batch
                while len(pending_predictions) < self.BATCH_SIZE:
                    try:
                        item = await asyncio.wait_for(self.prediction_queue.get(), timeout=0.1)
                        pending_predictions.append(item)
                    except asyncio.TimeoutError:
                        break
                
                if pending_predictions:
                    # Process batch
                    await self._process_prediction_batch(pending_predictions)
                    pending_predictions.clear()
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"‚ùå L·ªói trong batch processor: {e}")
                await asyncio.sleep(0.1)
    
    async def _process_prediction_batch(self, batch):
        """X·ª≠ l√Ω batch predictions"""
        try:
            # T√°ch data v√† metadata
            frames_batch = [item['frames'] for item in batch]
            
            # Batch prediction (n·∫øu model h·ªó tr·ª£)
            results = await self._batch_predict(frames_batch)
            
            # G·ª≠i k·∫øt qu·∫£ v·ªÅ cho t·ª´ng client
            for i, item in enumerate(batch):
                websocket = item['websocket']
                client_id = item['client_id']
                result = results[i] if i < len(results) else None
                
                try:
                    await self._send_result(websocket, client_id, result)
                except Exception as e:
                    print(f"G·ª≠i k·∫øt qu·∫£ cho client {client_id} th·∫•t b·∫°i: {e}")


        except Exception as e:
            print(f"‚ùå L·ªói batch prediction: {e}")
    
    async def _batch_predict(self, frames_batch):
        """Predict cho batch frames"""
        results = []
        
        # X·ª≠ l√Ω tu·∫ßn t·ª± (c√≥ th·ªÉ t·ªëi ∆∞u th√†nh parallel n·∫øu model h·ªó tr·ª£)
        for frames in frames_batch:
            try:
                result = await asyncio.wait_for(
                    predict_sequence_from_frames(frames), 
                    timeout=3
                )
                results.append(result)
            except asyncio.TimeoutError:
                results.append({
                    "label": "timeout",
                    "confidence": 0.0,
                    "message": "Prediction timeout"
                })
            except Exception as e:
                results.append({
                    "label": "error",
                    "confidence": 0.0,
                    "message": f"Prediction error: {str(e)}"
                })
        
        return results
    
    async def handle_continuous_prediction(self, websocket, client_id, frames, timestamp):
        """X·ª≠ l√Ω prediction li√™n t·ª•c v·ªõi t·ªëi ∆∞u"""
        current_time = time.time()
        
        # Throttle prediction per client
        last_time = self.client_prediction_times.get(client_id, 0)
        if current_time - last_time < self.MIN_PREDICTION_INTERVAL:
            return  # Skip n·∫øu qu√° g·∫ßn prediction tr∆∞·ªõc
        
        # Update buffer cho client
        self.client_buffers[client_id] = frames
        self.client_prediction_times[client_id] = current_time
        
        # Th√™m v√†o queue ƒë·ªÉ x·ª≠ l√Ω batch
        await self.prediction_queue.put({
            'websocket': websocket,
            'client_id': client_id,
            'frames': frames,
            'timestamp': timestamp
        })
    
    async def _send_result(self, websocket, client_id, result):
        """G·ª≠i k·∫øt qu·∫£ v·ªõi duplicate detection"""
        try:
            label = result["label"]
            confidence = result["confidence"]
            
            # Ki·ªÉm tra duplicate
            last_prediction = self.client_last_predictions.get(client_id)
            if (last_prediction and 
                last_prediction["label"] == label and 
                abs(last_prediction["confidence"] - confidence) < 0.05):
                return  # Skip duplicate
            
            # Cache prediction m·ªõi
            self.client_last_predictions[client_id] = {
                "label": label,
                "confidence": confidence,
                "timestamp": time.time()
            }
            
            # G·ª≠i k·∫øt qu·∫£
            if confidence > self.CONFIDENCE_THRESHOLD:
                await websocket.send(json.dumps({
                    "status": "success",
                    "label": label,
                    "confidence": confidence,
                    "type": "continuous_prediction",
                    "timestamp": time.time()
                }))
                print(f"‚úÖ Client {client_id}: {label} ({confidence:.2f})")
            else:
                await websocket.send(json.dumps({
                    "status": "low_confidence",
                    "label": label,
                    "confidence": confidence,
                    "type": "continuous_prediction"
                }))
                
        except Exception as e:
            print(f"‚ùå L·ªói g·ª≠i k·∫øt qu·∫£: {e}")
    
    def cleanup_client(self, client_id):
        """D·ªçn d·∫πp data c·ªßa client khi disconnect"""
        self.client_buffers.pop(client_id, None)
        self.client_last_predictions.pop(client_id, None)
        self.client_prediction_times.pop(client_id, None)

# Global manager
prediction_manager = ContinuousPredictionManager()

async def handler(websocket):
    client_id = id(websocket)  # Unique ID cho m·ªói client
    connected_clients.add(websocket)
    print(f"üü¢ Client {client_id} ƒë√£ k·∫øt n·ªëi")
    
    # Start batch processor n·∫øu ch∆∞a ch·∫°y
    await prediction_manager.start_batch_processor()

    try:
        async for message in websocket:
            data = json.loads(message)
            frames = data.get("frames")
            msg_type = data.get("type", "prediction")
            timestamp = data.get("timestamp", time.time())

            if frames and isinstance(frames, list) and all(len(f) == 126 for f in frames):
                
                if msg_type == "continuous_prediction":
                    # X·ª≠ l√Ω prediction li√™n t·ª•c
                    await prediction_manager.handle_continuous_prediction(
                        websocket, client_id, frames, timestamp
                    )
                else:
                    # X·ª≠ l√Ω prediction th√¥ng th∆∞·ªùng (legacy)
                    try:
                        result = await asyncio.wait_for(
                            predict_sequence_from_frames(frames), 
                            timeout=5
                        )
                        
                        label = result["label"]
                        confidence = float(result["confidence"])

                        if confidence > 0.9:
                            print(f"‚úÖ D·ª± ƒëo√°n: {label} ({confidence:.2f})")
                            await websocket.send(json.dumps({
                                "status": "success",
                                "label": label,
                                "confidence": confidence
                            }))
                        else:
                            await websocket.send(json.dumps({
                                "status": "error",
                                "label": label,
                                "confidence": confidence
                            }))
                            print(f"‚ö†Ô∏è Kh√¥ng ch·∫Øc ch·∫Øn {label} ({confidence:.2f})")
                            
                    except asyncio.TimeoutError:
                        print("‚ùå Predict qu√° l√¢u, b·ªè qua.")
                        await websocket.send(json.dumps({
                            "status": "timeout",
                            "label": "timeout",
                            "confidence": 0.0
                        }))
            else:
                print("‚ùå D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá. M·ªói frame ph·∫£i c√≥ 126 ph·∫ßn t·ª≠.")
                
    except websockets.exceptions.ConnectionClosed:
        print(f"üî¥ Client {client_id} ƒë√£ ng·∫Øt k·∫øt n·ªëi")
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω client {client_id}: {e}")
    finally:
        connected_clients.remove(websocket)
        prediction_manager.cleanup_client(client_id)
        
        # Stop batch processor n·∫øu kh√¥ng c√≤n client
        if not connected_clients:
            await prediction_manager.stop_batch_processor()

async def main():
    async with websockets.serve(handler, "0.0.0.0", 8765):
        print("üöÄ WebSocket server ƒëang ch·∫°y t·∫°i ws://localhost:8765")
        print("üìä Batch processing enabled cho continuous prediction")
        await asyncio.Future()

if __name__ == "__main__":
    asyncio.run(main())